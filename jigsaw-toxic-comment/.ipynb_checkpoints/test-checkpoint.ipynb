{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine what the data looks like.\n",
    "\n",
    "(Note: This repo does not contain the full data. To get the full data, go to the [Kaggle competition page](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) and download the data for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28e4f9d2f129d1b8</td>\n",
       "      <td>\"::My class went quite well today. A student t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f54b1d9a5d75ee75</td>\n",
       "      <td>I truly don't see the issue with the breakdown...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  28e4f9d2f129d1b8  \"::My class went quite well today. A student t...      0   \n",
       "1  f54b1d9a5d75ee75  I truly don't see the issue with the breakdown...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0             0        0       0       0              0     1  \n",
       "1             0        0       0       0              0     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/train_ds.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345481186836b351</td>\n",
       "      <td>Why have I been blocked. I haven't done anythi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d65f3e209e35b44c</td>\n",
       "      <td>In 1987 film Planes Trains and Automobiles a c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  345481186836b351  Why have I been blocked. I haven't done anythi...      0   \n",
       "1  d65f3e209e35b44c  In 1987 film Planes Trains and Automobiles a c...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0             0        0       0       0              0     1  \n",
       "1             0        0       0       0              0     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/valid_ds.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently we have to predict 6 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC ==    The title is fine as it is, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC ==    The title is fine as it is, ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/test_ds.csv\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Field class determines how the data is preprocessed and converted into a numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want comment_text field to be converted to lowercase, tokenized on whitespace, and preprocessed. So we tell that to the Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True, tensor_type=torch.cuda.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. The preprocessing of the labels is even easier, since they are already converted into a binary encoding.\n",
    "All we need to do is to tell the Field class that the labels are already processed. We do this by passing the use_vocab=False keyword to the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = Field(sequential=False, use_vocab=False, tensor_type=torch.cuda.ByteTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the TabularDataset class to read our data, since it is in csv format (TabularDataset handles csv, tsv, and json files as of now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train and validation data, we need to process the labels. The fields we pass in must be in the same order as the columns. For fields we don't use, we pass in a tuple where the second element is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.31 s, sys: 408 ms, total: 8.72 s\n",
      "Wall time: 8.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "        path=\"data\", # the root directory where the data lies\n",
    "        train='train_ds.csv', validation=\"valid_ds.csv\",\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test data, we don't have any labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.59 s, sys: 288 ms, total: 6.88 s\n",
      "Wall time: 6.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"comment_text\", TEXT)\n",
    "]\n",
    "\n",
    "tst = TabularDataset(\n",
    "        path=\"data/test_ds.csv\", # the file path\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TEXT field to convert words into integers, it needs to be told what the entire vocabulary is. To do this, we run TEXT.build_vocab, passing in the dataset to build the vocabulary on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 32 ms, total: 2.73 s\n",
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the vocab looks like.\n",
    "\n",
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 343496),\n",
       " ('to', 206144),\n",
       " ('of', 155888),\n",
       " ('and', 152384),\n",
       " ('a', 148039),\n",
       " ('i', 136933),\n",
       " ('you', 132362),\n",
       " ('is', 119338),\n",
       " ('that', 102607),\n",
       " ('in', 98348)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also instructive to take a look inside the Dataset. Datasets can be indexed like normal lists, so we'll look at the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7f11d78d3128>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset is an Example object that bundles the attributes of a single data point together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the comment text is already tokenized for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"::my', 'class', 'went']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0].comment_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now, let's build the Iterator which will allow us to load the data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**.\n",
    "\n",
    "When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time.\n",
    "\n",
    "The BucketIterator groups sequences of similar lengths together for each batch to minimize padding. Handy, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_sizes=(64, 64),\n",
    "        device=-1, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.batch.Batch at 0x7f117f988780>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'train', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, we don't want the data to be shuffled. This is why we'll be using a standard Iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = Iterator(tst, batch_size=64, device=-1, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the iterator returns a custom datatype called torchtext.data.Batch. This makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai). \n",
    "\n",
    "I hope this will be dealt with in the future (I'm considering filing a PR if I can decide what the API should look like), but in the meantime, we'll hack on a simple wrapper to make the batches easy to use. \n",
    "\n",
    "Concretely, we'll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this to wrap the BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "test_dl = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " \n",
       " Columns 0 to 5 \n",
       "  4.6700e+02  8.0000e+00  1.5190e+03  8.0000e+00  3.2720e+03  1.1000e+01\n",
       "  3.3511e+05  2.0600e+02  6.8247e+04  2.2278e+05  8.2000e+01  2.0000e+00\n",
       "  4.9610e+03  1.7150e+03  2.8800e+02  2.2700e+02  1.6000e+01  3.7840e+03\n",
       "  2.3729e+05  2.2000e+01  2.8000e+01  2.2276e+05  2.2000e+01  4.7200e+02\n",
       "  1.0470e+03  4.2000e+01  6.2190e+03  1.7057e+05  2.2300e+02  1.9490e+03\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00\n",
       " \n",
       " Columns 6 to 11 \n",
       "  3.7900e+02  4.7000e+01  3.6444e+05  4.6700e+02  1.2100e+03  4.2100e+02\n",
       "  1.3000e+01  8.0000e+00  6.0000e+00  1.3100e+02  7.8100e+02  1.1000e+02\n",
       "  9.4130e+03  2.5000e+02  3.3900e+02  3.3351e+05  4.9700e+02  5.2500e+02\n",
       "  5.0000e+00  8.0000e+00  3.0246e+05  6.6750e+03  1.6564e+05  2.1300e+02\n",
       "  6.9334e+04  1.2348e+04  2.8100e+02  2.9405e+05  1.9000e+02  1.2340e+03\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00\n",
       " \n",
       " Columns 12 to 17 \n",
       "  9.0000e+01  4.0000e+00  1.4193e+05  2.0000e+00  1.3700e+02  4.6700e+02\n",
       "  1.1270e+03  2.1980e+03  2.6762e+04  1.4500e+02  2.9800e+02  3.3478e+05\n",
       "  1.7433e+05  5.0000e+00  1.1920e+03  9.7000e+01  4.0868e+04  2.2590e+03\n",
       "  2.8000e+01  3.5226e+05  4.9330e+03  1.7000e+01  4.0868e+04  1.1000e+01\n",
       "  5.4780e+03  1.9820e+03  1.8000e+01  6.2240e+03  2.6578e+05  5.0081e+04\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00\n",
       " \n",
       " Columns 18 to 23 \n",
       "  1.8000e+01  3.9540e+03  2.8758e+05  7.9500e+02  7.0000e+00  2.8240e+03\n",
       "  5.4500e+03  1.6900e+02  9.0000e+00  3.0000e+00  2.5000e+01  3.3000e+01\n",
       "  2.8100e+02  3.8400e+02  2.0100e+02  2.8000e+01  5.3900e+02  3.0100e+02\n",
       "  1.8163e+04  3.0000e+00  1.4100e+02  1.9159e+04  3.0000e+00  5.4000e+01\n",
       "  1.8000e+01  8.5150e+03  3.5181e+05  5.7000e+01  1.5900e+02  2.3635e+04\n",
       "  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00  1.0000e+00\n",
       " \n",
       " Columns 24 to 29 \n",
       "  3.3170e+03  4.6700e+02  1.8888e+04  7.0000e+00  9.1000e+01  1.2820e+03\n",
       "  1.7800e+02  5.7360e+03  8.1000e+01  7.3000e+01  1.0600e+03  1.8200e+02\n",
       "  1.5000e+02  4.0000e+00  4.0000e+00  3.6700e+02  1.3856e+04  8.6000e+01\n",
       "  6.0000e+00  3.6840e+03  3.2920e+03  7.6500e+02  2.9000e+02  6.0000e+00\n",
       "  2.7624e+05  5.9309e+04  2.1170e+03  1.7000e+01  4.4500e+02  1.4660e+04\n",
       "  1.0000e+00  1.5780e+03  2.1000e+01  4.6312e+04  1.5810e+03  2.4459e+04\n",
       " \n",
       " Columns 30 to 35 \n",
       "  6.8000e+01  1.3000e+01  5.9300e+02  4.6100e+02  2.3750e+04  1.1600e+04\n",
       "  7.7000e+01  6.0000e+00  5.6900e+02  1.5000e+01  5.0000e+01  2.6534e+05\n",
       "  5.3850e+03  5.9880e+03  1.4150e+03  9.0000e+00  5.4000e+01  3.8700e+02\n",
       "  3.3640e+04  1.1050e+03  8.8000e+01  6.0000e+00  9.7300e+02  5.0480e+03\n",
       "  2.8629e+04  8.5000e+01  1.4150e+03  1.5700e+02  6.0000e+00  2.0000e+00\n",
       "  1.1240e+03  2.4726e+05  8.8000e+01  1.2200e+02  5.9722e+04  1.3230e+03\n",
       " \n",
       " Columns 36 to 41 \n",
       "  9.0000e+00  2.1100e+02  1.0827e+05  6.8612e+04  4.6700e+02  1.4258e+05\n",
       "  6.0000e+00  3.8577e+04  7.2270e+03  1.1382e+05  3.3469e+05  2.1800e+05\n",
       "  5.4000e+01  9.5000e+02  1.0330e+03  1.4400e+02  1.2728e+04  1.6000e+01\n",
       "  4.7200e+02  3.3470e+03  4.2240e+03  2.0000e+00  1.5371e+05  7.2970e+03\n",
       "  2.5780e+03  7.3200e+02  5.0537e+04  5.8000e+01  2.3174e+04  5.1310e+03\n",
       "  8.1300e+02  1.9000e+02  2.8808e+04  1.2200e+02  7.2069e+04  1.8000e+01\n",
       " \n",
       " Columns 42 to 47 \n",
       "  8.0000e+00  5.9590e+03  1.8000e+01  2.4800e+02  4.3697e+04  1.0000e+02\n",
       "  1.0200e+02  3.6000e+01  1.0800e+02  2.3880e+03  1.0920e+05  1.6400e+02\n",
       "  1.4030e+04  8.0000e+00  6.2500e+02  3.2000e+01  2.0000e+01  5.0000e+00\n",
       "  7.0000e+01  5.3150e+03  3.9400e+02  5.6000e+01  2.2450e+03  2.0000e+01\n",
       "  9.0000e+00  4.8000e+01  3.7170e+03  5.7580e+03  2.8000e+01  2.3500e+03\n",
       "  1.2530e+03  2.9918e+05  1.8000e+01  7.8000e+02  1.0770e+05  4.1100e+02\n",
       " \n",
       " Columns 48 to 53 \n",
       "  2.8000e+01  3.4790e+03  7.2000e+01  1.8000e+01  4.6700e+02  2.0000e+00\n",
       "  1.6000e+02  1.7400e+02  5.2000e+01  5.2610e+03  3.3488e+05  1.3800e+02\n",
       "  3.7000e+01  8.3000e+01  1.4180e+03  2.6560e+03  9.0980e+03  7.0000e+00\n",
       "  2.4000e+01  3.0000e+01  3.8000e+01  1.2599e+05  2.8261e+04  4.1900e+02\n",
       "  7.0000e+00  2.0000e+00  1.2800e+03  2.5600e+02  1.1990e+03  1.0700e+02\n",
       "  1.2010e+05  7.3500e+02  8.1500e+02  1.8000e+01  1.8480e+03  9.5583e+04\n",
       " \n",
       " Columns 54 to 59 \n",
       "  6.2600e+02  1.5390e+03  4.6700e+02  8.4000e+01  1.3590e+03  2.2000e+01\n",
       "  5.1000e+01  2.8100e+02  6.0348e+04  7.0000e+00  1.2000e+01  2.2100e+02\n",
       "  4.0000e+01  6.4120e+03  6.1100e+02  6.3600e+02  8.8000e+01  3.0000e+00\n",
       "  5.5830e+03  6.0400e+02  6.4200e+03  7.0000e+00  2.2000e+01  1.7000e+01\n",
       "  6.6000e+01  1.0170e+03  3.7420e+03  8.4100e+02  9.1005e+04  2.5300e+03\n",
       "  8.2490e+03  5.2320e+03  2.3750e+03  9.7760e+03  8.1300e+02  1.7838e+04\n",
       " \n",
       " Columns 60 to 63 \n",
       "  1.1930e+03  1.8400e+02  6.0570e+03  1.5836e+05\n",
       "  3.3900e+02  1.5941e+04  7.5200e+02  6.8100e+02\n",
       "  1.0504e+05  8.5650e+03  7.8554e+04  1.0830e+03\n",
       "  3.5950e+03  3.9000e+01  4.0000e+00  8.0000e+00\n",
       "  4.0000e+00  6.3000e+01  2.0000e+00  4.9000e+01\n",
       "  2.9800e+02  8.2130e+03  5.6917e+04  9.3900e+02\n",
       " [torch.cuda.LongTensor of size 6x64 (GPU 0)], Variable containing:\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     1     1     0\n",
       "     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     1     0     0     1     1     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     1     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     1     1     1\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     1     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     1     0     0     0     0     0\n",
       "     1     0     0     1     1     0\n",
       "     0     0     0     0     0     0\n",
       "     1     1     0     1     1     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       "     0     0     0     0     0     0\n",
       " [torch.cuda.FloatTensor of size 64x6 (GPU 0)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start training a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple LSTM as a baseline example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout, )\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "#         pdb.set_trace()\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds #F.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(368025, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
       "  )\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz, num_linear=nl); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(368025, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
       "  )\n",
       "  (predictor): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "# loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1745/1746 [01:59<00:00, 14.66it/s]/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n",
      "100%|██████████| 1746/1746 [01:59<00:00, 14.66it/s]\n",
      "  0%|          | 0/1746 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.1404, Validation Loss: 0.7510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1746/1746 [01:59<00:00, 14.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.0578, Validation Loss: 0.7673\n",
      "CPU times: user 3min 7s, sys: 1min 13s, total: 4min 21s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x, y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.data[0] * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    # calculate the validation loss for this epoch\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(y, preds)\n",
    "        val_loss += loss.data[0] * x.size(0)\n",
    "\n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output the data in the format required by the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BatchWrapper at 0x114438390>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]/Users/keitakurita/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_dl):\n",
    "    preds = model(x)\n",
    "    # if you're data is on the GPU, you need to move the data back to the cpu\n",
    "    # preds = preds.data.cpu().numpy()\n",
    "    preds = preds.data.numpy()\n",
    "    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    df[col] = test_preds[:, i]\n",
    "\n",
    "# if you want to write the submission file to disk, uncomment and run the below code\n",
    "# df.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>8.733038</td>\n",
       "      <td>4.913491</td>\n",
       "      <td>-0.283496</td>\n",
       "      <td>5.04092</td>\n",
       "      <td>5.580082</td>\n",
       "      <td>-0.449243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>8.733038</td>\n",
       "      <td>4.913491</td>\n",
       "      <td>-0.283496</td>\n",
       "      <td>5.04092</td>\n",
       "      <td>5.580082</td>\n",
       "      <td>-0.449243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>8.733038</td>\n",
       "      <td>4.913491</td>\n",
       "      <td>-0.283496</td>\n",
       "      <td>5.04092</td>\n",
       "      <td>5.580082</td>\n",
       "      <td>-0.449243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "\n",
       "      toxic  severe_toxic   obscene   threat    insult  identity_hate  \n",
       "0  8.733038      4.913491 -0.283496  5.04092  5.580082      -0.449243  \n",
       "1  8.733038      4.913491 -0.283496  5.04092  5.580082      -0.449243  \n",
       "2  8.733038      4.913491 -0.283496  5.04092  5.580082      -0.449243  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
