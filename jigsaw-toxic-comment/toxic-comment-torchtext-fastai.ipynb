{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.rnn_reg import EmbeddingDropout, WeightDrop, LockedDropout\n",
    "from fastai.torch_imports import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_STOPWORDS\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# pandas and plotting config\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "\n",
    "os.makedirs(f'{PATH}/models', exist_ok=True)\n",
    "os.makedirs(f'{PATH}/tmp', exist_ok=True)\n",
    "os.makedirs(f'{PATH}/submissions', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and define labels (ordering is important for competition submission!)\n",
    "\n",
    "*Note: We are also adding a \"None\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Clean comments using techniques from other kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.read_csv(f'{PATH}/train.csv')\n",
    "test_df = pd.read_csv(f'{PATH}/test.csv')\n",
    "sample_subm_df = pd.read_csv(f'{PATH}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "raw_train_df['none'] = 1 - raw_train_df[label_cols].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df.comment_text.fillna(\"<na>\", inplace=True)\n",
    "test_df.comment_text.fillna(\"<na>\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     83
    ]
   },
   "outputs": [],
   "source": [
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "}\n",
    "\n",
    "#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n",
    "# Aphost lookup dict\n",
    "appos = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'd\" : \"I had\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"tryin'\": \"trying\"\n",
    "}\n",
    "\n",
    "repl = { **appos, **repl }  # repl becomes a merged dictionary with values from repl replacing those from appos\n",
    "\n",
    "# display(repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_keys = [i for i in repl.keys()]\n",
    "\n",
    "def clean(comment):\n",
    "    # convert to lower case , so that Hi and hi are the same\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    # remove \\n \n",
    "    # torchtext cannot read the .csv files correctly if there are newline characters, so replace with \" \"\n",
    "    comment = re.sub(\"\\\\n\",\" \",comment)\n",
    "    \n",
    "    # remove leaky elements like ip,user\n",
    "    comment = re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\" \",comment)\n",
    "    \n",
    "    # removing usernames\n",
    "    comment = re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    # do any substitutions\n",
    "    comment = \" \".join([ repl[w] if (w in repl_keys) else w for w in comment.split() ])\n",
    "    \n",
    "    return(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 64 ms, total: 19.4 s\n",
      "Wall time: 19.4 s\n",
      "train cleaned ...\n",
      "CPU times: user 17.1 s, sys: 64 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n",
      "test cleaned ...\n"
     ]
    }
   ],
   "source": [
    "%time raw_train_df['comment_text_cleaned'] = raw_train_df.comment_text.apply(lambda x: clean(x))\n",
    "print('train cleaned ...')\n",
    "\n",
    "%time test_df['comment_text_cleaned'] = test_df.comment_text.apply(lambda x: clean(x))\n",
    "print('test cleaned ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df.to_csv(f'{PATH}/train_preproc.csv', index=None)\n",
    "test_df.to_csv(f'{PATH}/test_preproc.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the preprocessed datasets for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df = pd.read_csv(f'{PATH}/train_preproc.csv')\n",
    "test_df = pd.read_csv(f'{PATH}/test_preproc.csv')\n",
    "sample_subm_df = pd.read_csv(f'{PATH}/sample_submission.csv')\n",
    "\n",
    "txt_col = 'comment_text_cleaned'\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "raw_train_df['none'] = 1 - raw_train_df[label_cols].max(axis=1)\n",
    "\n",
    "model_cols = ['id', txt_col] + label_cols + ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "n_trn = len(raw_train_df)\n",
    "n_examples_per_fold = n_trn // n_folds\n",
    "\n",
    "# n_examples_per_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "*Note: Only need to run 1x (or as desired to regenerate these .csv files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build cross-validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df_rand = raw_train_df.sample(frac=1, random_state=9) # frac=1 = return all rows in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dfs = []\n",
    "\n",
    "for i in range(0, n_folds):\n",
    "    start = i * n_examples_per_fold\n",
    "    end = n_examples_per_fold + start if (i + 1 < n_folds) else None\n",
    "    val_dfs.append(raw_train_df_rand[start:end])\n",
    "    \n",
    "# [ print(idx,len(d)) for idx, d in enumerate(val_dfs) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dfs = []\n",
    "\n",
    "for idx, df in enumerate(val_dfs):\n",
    "    trn_dfs.append(pd.concat([ val_df for val_idx, val_df in enumerate(val_dfs) if val_idx != idx]))\n",
    "    \n",
    "# [ print(idx,len(d)) for idx, d in enumerate(trn_dfs) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143614 15957\n",
      "1 143614 15957\n",
      "2 143614 15957\n",
      "3 143614 15957\n",
      "4 143614 15957\n",
      "5 143614 15957\n",
      "6 143614 15957\n",
      "7 143614 15957\n",
      "8 143614 15957\n",
      "9 143613 15958\n"
     ]
    }
   ],
   "source": [
    "for idx, [trn_df, val_df] in enumerate(zip(trn_dfs, val_dfs)):\n",
    "    print(idx, len(trn_df), len(val_df))\n",
    "    \n",
    "    trn_df[model_cols].to_csv(f'{PATH}/train_ds_{idx}_of_{n_folds}.csv', index=None)\n",
    "    val_df[model_cols].to_csv(f'{PATH}/valid_ds_{idx}_of_{n_folds}.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below if you want to create a single training and cv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151592 7979 15417 808\n"
     ]
    }
   ],
   "source": [
    "# split the training data into a train and validatin dataset\n",
    "trn, val = train_test_split(raw_train_df, test_size=0.05, random_state=9)\n",
    "print(len(trn), len(val), len(trn[trn.none != 1]), len(val[val.none != 1]))\n",
    "\n",
    "# save train, val, and test datasets for torchtext\n",
    "trn[model_cols].to_csv(f'{PATH}/train_ds.csv', index=None)\n",
    "val[model_cols].to_csv(f'{PATH}/valid_ds.csv', index=None)\n",
    "\n",
    "# save full cleaned datasets (train+valid and test) as well\n",
    "raw_train_df[model_cols].to_csv(f'{PATH}/full_train_ds.csv', index=None)\n",
    "test_df[['id', txt_col]].to_csv(f'{PATH}/test_ds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text_cleaned</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my username hardcore metallica fan were reverted? they were not vandalisms, just closure on some gas after i voted at new york dolls fac. and please do not remove the template from the talk page since i am retired now.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d'aww! he matches this background colour i am seemingly stuck with. thanks. (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  0000997932d777bf   \n",
       "1  000103f0d9cfb60f   \n",
       "\n",
       "                                                                                                                                                                                                                                              comment_text_cleaned  \\\n",
       "0  explanation why the edits made under my username hardcore metallica fan were reverted? they were not vandalisms, just closure on some gas after i voted at new york dolls fac. and please do not remove the template from the talk page since i am retired now.   \n",
       "1  d'aww! he matches this background colour i am seemingly stuck with. thanks. (talk) 21:51, january 11, 2016 (utc)                                                                                                                                                  \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0  0      0             0        0       0       0              1     \n",
       "1  0      0             0        0       0       0              1     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule is more succesful then you will ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== from rfc == the title is fine as it is, imo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  00001cee341fdb12   \n",
       "1  0000247867823ef7   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                comment_text_cleaned  \n",
       "0  yo bitch ja rule is more succesful then you will ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,  \n",
       "1  == from rfc == the title is fine as it is, imo.                                                                                                                                                                                                                                                                                                                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.read_csv(\"data/full_train_ds.csv\").head(2))\n",
    "display(pd.read_csv(\"data/test_ds.csv\").head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Datasets and DataLoaders\n",
    "\n",
    "Define hyperparameters and column that holds text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000 #30000\n",
    "min_freq = 10 #0\n",
    "max_len = 175 #100\n",
    "\n",
    "pretrained_vectors = None #'fasttext.en.300d'\n",
    "\n",
    "batch_sizes = (64,64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure how we are going to process text and label fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_fld = data.Field(sequential=True, tokenize=tokenize, lower=True, fix_length=max_len)\n",
    "LABEL_fld = data.Field(sequential=False, use_vocab=False, tensor_type=torch.cuda.ByteTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various built-in Datasets in torchtext that handle common use cases. **For csv/tsv files, the TabularDataset class** is convenient. Here’s how we would read data from a csv file using the TabularDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 1.44 s, total: 43.7 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# train/validation\n",
    "train_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                    (txt_col, TEXT_fld),\n",
    "                    (\"toxic\", LABEL_fld), (\"severe_toxic\", LABEL_fld), (\"obscene\", LABEL_fld),\n",
    "                    (\"threat\", LABEL_fld), (\"insult\", LABEL_fld), (\"identity_hate\", LABEL_fld), (\"none\", None)]\n",
    "\n",
    "train_ds, valid_ds = data.TabularDataset.splits(PATH, train='train_ds.csv', validation='valid_ds.csv',\n",
    "                                          format='csv', skip_header=True, fields=train_datafields)\n",
    "\n",
    "# test\n",
    "test_datafields = [(\"id\", None), (txt_col, TEXT_fld)]\n",
    "test_ds = data.TabularDataset(f'{PATH}/test_ds.csv', format='csv', skip_header=True, fields=test_datafields)\n",
    "\n",
    "# train+val\n",
    "full_train_ds = data.TabularDataset(f'{PATH}/full_train_ds.csv', \n",
    "                                    format='csv', skip_header=True, fields=train_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x7faaa62d9d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text_cleaned', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['you', 'cunt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_ds[0])\n",
    "display(train_ds[0].__dict__.keys())\n",
    "display(train_ds[1].comment_text_cleaned[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocab on *full* training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_fld.build_vocab(full_train_ds, min_freq=min_freq, max_size=max_features, vectors=pretrained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 645337),\n",
       " ('the', 496748),\n",
       " (',', 473218),\n",
       " ('\"', 392156),\n",
       " ('to', 297318),\n",
       " ('i', 240305),\n",
       " ('of', 224837),\n",
       " ('and', 224115),\n",
       " ('is', 222448),\n",
       " ('you', 221861)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words.\n",
    "TEXT_fld.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    (train_ds, valid_ds), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(batch_sizes[0], batch_sizes[1]),\n",
    "    device=0, #-1 if CPU else GPU number if you want to use the GPU\n",
    "    sort_key=lambda x: len(x.comment_text_cleaned), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False,\n",
    "    repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.batch.Batch at 0x7faaa796f898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'train', 'comment_text_cleaned', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); \n",
    "\n",
    "display(batch)\n",
    "display(batch.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, we don't want the data to be shuffled. This is why we'll be using a standard Iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = data.Iterator(test_ds, batch_size=batch_sizes[2], device=0, train=False, \n",
    "                          shuffle=False, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            \n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([ getattr(batch, feat).unsqueeze(1) for feat in self.y_vars ], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, txt_col, label_cols)\n",
    "valid_dl = BatchWrapper(val_iter, txt_col, label_cols)\n",
    "test_dl = BatchWrapper(test_iter, txt_col, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a fastai ModelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData(PATH, trn_dl=train_dl, val_dl=valid_dl, test_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([175, 64]), torch.Size([64, 6]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models\n",
    "\n",
    "Define a simple GRU and a simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleGru(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz=300, n_rnn_hidden=256, n_rnn_layers=1, bi_dir=True, out_sz=1, bsz=64,\n",
    "                 dropout_rnn=0.3, dropout_after_emb=0.4, dropout_emb=0.1, wdrop=0.05):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.bsz = bsz\n",
    "               \n",
    "        # configure embeddings layer\n",
    "        self.dropout_emb = dropout_emb\n",
    "        self.dropout_after_emb = LockedDropout(dropout_after_emb)\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz)\n",
    "        self.emb.data = train_ds.fields[txt_col].vocab.vectors # to use the pretrained vectors\n",
    "        self.emb_with_drop = EmbeddingDropout(self.emb)\n",
    "        \n",
    "        # configure rnns\n",
    "        self.n_rnn_hidden, self.n_rnn_layers, self.n_dirs = n_rnn_hidden, n_rnn_layers, 2 if bi_dir else 1\n",
    "        self.rnn = nn.GRU(emb_sz, self.n_rnn_hidden, self.n_rnn_layers, bidirectional=bi_dir, dropout=dropout_rnn)\n",
    "        if wdrop: self.rnn = WeightDrop(self.rnn, wdrop)\n",
    "      \n",
    "        self.outp = nn.Linear(n_rnn_hidden * 2 * self.n_dirs, out_sz)\n",
    "        \n",
    "        # initialize weights\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "        \n",
    "        # init hidden\n",
    "        self.init_hidden(self.bsz)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        bsz = seq.size(1)\n",
    "        if (self.hidden[0].size(1) != bsz): self.init_hidden(bsz)\n",
    "        \n",
    "        x = self.emb_with_drop(seq, dropout=self.dropout_emb if self.training else 0)\n",
    "        x = self.dropout_after_emb(x)\n",
    "        \n",
    "        output, h = self.rnn(x, self.hidden)        \n",
    "        self.hidden = repackage_var(h)\n",
    "        \n",
    "        sl, bs, _ = output.size()\n",
    "  \n",
    "        avg_pool = F.adaptive_avg_pool1d(output.permute(1,2,0), (1,)).view(bs,-1)   \n",
    "        max_pool = F.adaptive_max_pool1d(output.permute(1,2,0), (1,)).view(bs,-1) \n",
    "        \n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        outp = F.sigmoid(self.outp(x))\n",
    "        \n",
    "        return outp\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        self.hidden = V(torch.zeros(self.n_dirs * self.n_rnn_layers, bsz, self.n_rnn_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SimpleLstm(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz=300, n_rnn_hidden=256, n_rnn_layers=1, bi_dir=True, out_sz=1, bsz=64,\n",
    "                 dropout_rnn=0.3, dropout_after_emb=0.4, dropout_emb=0.1, wdrop=0.05):\n",
    "        \n",
    "        super().__init__() \n",
    "        \n",
    "        self.bsz = bsz\n",
    "               \n",
    "        # configure embeddings layer\n",
    "        self.dropout_emb = dropout_emb\n",
    "        self.dropout_after_emb = LockedDropout(dropout_after_emb)\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz)\n",
    "        self.emb.data = train_ds.fields[txt_col].vocab.vectors # to use the pretrained vectors\n",
    "        self.emb_with_drop = EmbeddingDropout(self.emb)\n",
    "        \n",
    "        # configure rnns\n",
    "        self.n_rnn_hidden, self.n_rnn_layers, self.n_dirs = n_rnn_hidden, n_rnn_layers, 2 if bi_dir else 1\n",
    "        self.rnn = nn.LSTM(emb_sz, self.n_rnn_hidden, self.n_rnn_layers, bidirectional=bi_dir, dropout=dropout_rnn)\n",
    "        if wdrop: self.rnn = WeightDrop(self.rnn, wdrop)\n",
    "      \n",
    "        self.outp = nn.Linear(n_rnn_hidden * 2 * self.n_dirs, out_sz)\n",
    "        \n",
    "        # initialize weights\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "        \n",
    "        # init hidden\n",
    "        self.init_hidden(self.bsz)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        bsz = seq.size(1)\n",
    "        if (self.hidden[0].size(1) != bsz): self.init_hidden(bsz)\n",
    "        \n",
    "        x = self.emb_with_drop(seq, dropout=self.dropout_emb if self.training else 0)\n",
    "        x = self.dropout_after_emb(x)\n",
    "        \n",
    "        output, h = self.rnn(x, self.hidden)        \n",
    "        self.hidden = repackage_var(h)\n",
    "        \n",
    "        sl, bs, _ = output.size()\n",
    "  \n",
    "        avg_pool = F.adaptive_avg_pool1d(output.permute(1,2,0), (1,)).view(bs,-1)   \n",
    "        max_pool = F.adaptive_max_pool1d(output.permute(1,2,0), (1,)).view(bs,-1) \n",
    "        \n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        outp = F.sigmoid(self.outp(x))\n",
    "        \n",
    "        return outp\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        self.hidden = (V(torch.zeros(self.n_dirs * self.n_rnn_layers, bsz, self.n_rnn_hidden)),\n",
    "                       V(torch.zeros(self.n_dirs * self.n_rnn_layers, bsz, self.n_rnn_hidden)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `SimpleLstm` model, experimenting with various hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLstm(\n",
       "  (dropout_after_emb): LockedDropout(\n",
       "  )\n",
       "  (emb): Embedding(26970, 300)\n",
       "  (emb_with_drop): EmbeddingDropout(\n",
       "    (embed): Embedding(26970, 300)\n",
       "  )\n",
       "  (rnn): WeightDrop(\n",
       "    (module): LSTM(300, 128, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (outp): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sz = len(TEXT_fld.vocab)\n",
    "emb_sz = 300\n",
    "out_sz = 6\n",
    "\n",
    "n_rnn_hidden = 128\n",
    "n_rnn_layers = 1\n",
    "bi_dir = True\n",
    "\n",
    "model = SimpleLstm(vocab_sz, emb_sz, n_rnn_hidden, n_rnn_layers, True, out_sz, bsz=batch_sizes[0])\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Utilize fastai callbacks to add weight-decay and SGDR with restarts goodness.  You can experiment with other callbacks here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}/models/lstm_fit_1_cyc_{cycle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the fastai `fit()` method to train the model.  This is our training/validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 381/2369 [00:40<03:28,  9.51it/s, loss=0.0631]"
     ]
    }
   ],
   "source": [
    "fit(model, md, 2**4-1, lo.opt, F.binary_cross_entropy, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{PATH}/models/lstm_fit_1_cyc_3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}/models/lstm_fit_2_cyc_{cycle}')\n",
    "\n",
    "lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)\n",
    "cb = [CosAnneal(lo, (len(md.trn_dl) * 20), on_cycle_end=on_end)]\n",
    "\n",
    "fit(model, md, 20, lo.opt, F.binary_cross_entropy, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py:357: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(model, test_dl)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,</td>\n",
       "      <td>0.991044</td>\n",
       "      <td>0.360929</td>\n",
       "      <td>0.968881</td>\n",
       "      <td>0.031345</td>\n",
       "      <td>0.943214</td>\n",
       "      <td>0.163108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is, IMO.</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.001279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  00001cee341fdb12   \n",
       "1  0000247867823ef7   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                      comment_text  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,   \n",
       "1  == From RfC == \\n\\n The title is fine as it is, IMO.                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0  0.991044  0.360929      0.968881  0.031345  0.943214  0.163108       \n",
       "1  0.007682  0.000560      0.002847  0.000853  0.002909  0.001279       "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    subm_df[col] = preds[:, i]\n",
    "\n",
    "subm_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to write the submission file to disk, uncomment and run the below code\n",
    "subm_df.drop(['comment_text'], axis=1).to_csv(f'{PATH}/submissions/lstm_subm_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "We can perform a k-fold cross validation by following the same steps on the base training/validation datasets, except this time we use the CV training/validation datasets we created up top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                    (txt_col, TEXT_fld),\n",
    "                    (\"toxic\", LABEL_fld), (\"severe_toxic\", LABEL_fld), (\"obscene\", LABEL_fld),\n",
    "                    (\"threat\", LABEL_fld), (\"insult\", LABEL_fld), (\"identity_hate\", LABEL_fld), (\"none\", None)]\n",
    "\n",
    "# test\n",
    "test_datafields = [(\"id\", None), (txt_col, TEXT_fld)]\n",
    "\n",
    "# define test dataset and iterator\n",
    "test_ds = data.TabularDataset(f'{PATH}/test_ds.csv', format='csv', skip_header=True, fields=test_datafields)\n",
    "test_iter = data.Iterator(test_ds, batch_size=batch_sizes[2], device=0, train=False, \n",
    "                          shuffle=False, sort=False, sort_within_batch=False, repeat=False)\n",
    "test_dl = BatchWrapper(test_iter, txt_col, None)\n",
    "\n",
    "# define FULL train dataset for building vocab\n",
    "full_train_ds = data.TabularDataset(f'{PATH}/full_train_ds.csv', \n",
    "                                    format='csv', skip_header=True, fields=train_datafields)\n",
    "\n",
    "TEXT_fld.build_vocab(full_train_ds, min_freq=min_freq, max_size=max_features, vectors=pretrained_vectors)\n",
    "\n",
    "# cv\n",
    "for i in range(n_folds):\n",
    "    print('-' * 10)\n",
    "    print(f'Fold {i} ....')\n",
    "    \n",
    "    # train/validation datsets\n",
    "    train_ds, valid_ds = data.TabularDataset.splits(PATH, \n",
    "                                                    train=f'train_ds_{i}_of_{n_folds}.csv', \n",
    "                                                    validation=f'valid_ds_{i}_of_{n_folds}.csv',\n",
    "                                                    format='csv', skip_header=True, fields=train_datafields)\n",
    "\n",
    "    # train/validation iterators/dataloaders\n",
    "    train_iter, val_iter = data.BucketIterator.splits(\n",
    "        (train_ds, valid_ds), # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_sizes=(batch_sizes[0], batch_sizes[1]),\n",
    "        device=0, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.comment_text_cleaned), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False) # we pass repeat=False because we want to wrap this Iterator layer.\n",
    "        \n",
    "    train_dl = BatchWrapper(train_iter, txt_col, label_cols)\n",
    "    valid_dl = BatchWrapper(val_iter, txt_col, label_cols)\n",
    "        \n",
    "    md = ModelData(PATH, trn_dl=train_dl, val_dl=valid_dl, test_dl=test_dl)\n",
    "    \n",
    "    model = SimpleLstm(vocab_sz, emb_sz, n_rnn_hidden, n_rnn_layers, True, out_sz, bsz=batch_sizes[0])    \n",
    "    model.cuda()\n",
    "    \n",
    "    lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)\n",
    "    on_end = lambda sched, cycle: save_model(model, f'{PATH}/models/fit_1_cv{i}_cyc_{cycle}')\n",
    "    cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2)] #, on_cycle_end=on_end)]\n",
    "    fit(model, md, 2**4-1, lo.opt, F.binary_cross_entropy, callbacks=cb)\n",
    "        \n",
    "    on_end = lambda sched, cycle: save_model(model, f'{PATH}/models/fit_2_cv{i}_cyc_{cycle}')\n",
    "    lo = LayerOptimizer(optim.Adam, model, 1e-3, 1e-5)\n",
    "    cb = [CosAnneal(lo, (len(md.trn_dl) * 6))] #, on_cycle_end=on_end)]\n",
    "    fit(model, md, 6, lo.opt, F.binary_cross_entropy, callbacks=cb)\n",
    "        \n",
    "    preds = predict(model, test_dl)\n",
    "        \n",
    "    subm_df = pd.read_csv(\"data/test.csv\")\n",
    "    for lbl_idx, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "        subm_df[col] = preds[:, lbl_idx]\n",
    "        \n",
    "    # if you want to write the submission file to disk, uncomment and run the below code\n",
    "    subm_df.drop(['comment_text'], axis=1).to_csv(f'{PATH}/submissions/subm_lstm_cv_{i}_001.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dfs = []\n",
    "for i in range(n_folds):\n",
    "    df = pd.read_csv(f'{PATH}/submissions/subm_lstm_cv_{i}_001.csv')\n",
    "    cv_dfs.append(df)\n",
    "    \n",
    "final_cv_df = pd.concat([ df for df in cv_dfs ])\n",
    "\n",
    "display(len(final_cv_df))\n",
    "display(final_cv_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cv_df = final_cv_df.groupby(['id']).mean().reset_index()\n",
    "final_cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cv_df.to_csv(f'{PATH}/submissions/subm_lstm_cv_final_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
